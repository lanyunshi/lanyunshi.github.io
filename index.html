<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yunshi Lan</title>
    <meta content="Yunshi Lan, https://lanyunshi.github.io" name="keywords">

    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>


    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-164510176-1');
    </script>

</head>


<body>

    <!--photo and basic information-->
    <div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
        <div style="margin: 0px auto; width: 100%;">
            <img title="yunshi" style="float: left; padding-left: .01em; height: 130px;"
                 src="./resources/images/me/me.jpg">
            <div style="padding-left: 12em; vertical-align: top; height: 120px;">
                <span style="line-height: 150%; font-size: 20pt;">Yunshi Lan (兰韵诗)</span><br>
                <span><a href="http://dase.ecnu.edu.cn">School of Data Science and Engineering</a>, <a
                        href="https://www.ecnu.edu.cn">East China Normal University
                            (ECNU)</a></span><br>
                <span><strong>Address</strong>: 3663 North ZhongshanRd, Putuo District, Shanghai, China</span><br>
                <span><strong>Office</strong>: Room 203, Geography Building</span><br>
                <span><strong>Email</strong>: yslan [at] dase.ecnu.edu.cn</span> <br>
            </div>
        </div>
    </div>
    <!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

    
    <!--biography-->
    <div style="clear: both;">
        <div class="section">
            <h2>About Me (<a href="https://github.com/lanyunshi">[GitHub]</a>
                <a href="https://scholar.google.com/citations?user=Q0F92XIAAAAJ&hl=en">[Google Scholar]</a>
                <!-- <a href="./resources/cv/.pdf">[CV]</a>) -->
            </h2>
            <div class="paper">
                I am currently an Associate Professor at the <a href="https://faculty.ecnu.edu.cn/_s37/lys2/main.psp">School of Data Science & Engineering</a>, East China Normal University (ECNU). Before that, I was a research scientist at LARC, Singapore Management University (SMU). I obtained my Ph.D degree from School of Computing & Information System, Singapore Management University. I was trained (by my amazing supervisor <a href="http://www.mysmu.edu/faculty/jingjiang/">Prof. Jing Jiang</a> and secondary supervisor <a href="https://scholar.google.com/citations?user=uLa0zdcAAAAJ&hl=en">Prof. Feida Zhu)</a> to be a NLP researcher. I received my Bachelor degree from the School of Mathematics and Statistics, Southwest University (SWU).
                <br><br>
            </div>
        </div>
    </div>

    <!--Research Interest-->
    <div style="clear: both;">
        <div class="section">
            <h2>Research Interest
            </h2>
            <div class="paper">
                My recent research interest mainly includes knowledge bases, question answering, information extraction, text generation, deep learning and large language models.<br>
                <alert>I am looking for self-motivated students with strong programming skill to work with me. Please read this <a href="./resources/html/faq.html">FAQ</a> before emailing me!</alert>
                <br>
                <alert>2024年研究生名额已满，25年获得华师大数据学院保研/考研录用的同学欢迎联系我</alert><br>
            </div>
        </div>
    </div>


    <!--News-->
    <div style="clear: both;">
        <div class="section">
            <h2 id="news">News</h2>
                <ul>
                    <li>2024-07: One paper about multi-modal LLMs was accepted by <a href="https://eccv.ecva.net">ECCV 2024</a>.</li>
                    <li>2024-05: One demo paper about Finance QA system was accepted by <a href="https://ecmlpkdd.org/2024/">ECML PKDD 2024</a>.</li>
                    <li>2024-04: One survey paper about multi-modal LLMs was accepted by <a href="https://ijcai24.org">IJCAI 2024</a>.</li>
                    <li>2024-03: One paper about automated scoring was accepted by <a href="https://aied2024.cesar.school">AIED 2024</a>.</li>
                    <li>2024-02: One paper about text simplification was accepted by <a href="https://lrec-coling-2024.org/about-lrec-coling/">COLING 2024</a>.</li>                
                </ul>
            </details>

            <details>
                <summary><span style="color:blue">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2023]</span></summary>
                <ul>
                    <!--<br>-->
                    <li>2023-12: We won the Best Innovation Award in <a href="https://mp.weixin.qq.com/s/sfJPI5yIIE9igTZlpbU5QQ">Yangtze River Delta Fintech Innovation & Application Global Competition</a>.</li>
                    <li>2023-10: Two paper about Question Answering and Chain-of-Thought were accepted by EMNLP 2023.</li>
                    <li>2023-09: One paper about FL was accepted by NeurIPS 2023.</li>
                    <li>2023-08: One paper about Chinese Grammar Error Correction dataset was accepted by ACM CIKM 2023.</li>
                    <li>2023-07: One paper about Knowledge Base Visual Question Answering was accepted by ACM MM 2023.</li>
                    <li>2023-06: Rank First in the evaluation of NLPCC2023 Shared Task 8: Chinese Spelling Check.</li>
                    <li>2023-05: Three papers about Question Answering, Math Word Problem Solving were accepted by ACL 2023.</li>
                    <li>2023-04: One paper about FL was accepted by CVPR 2023.</li>
                </ul>
            </details>
        </div>
    </div>

    <!--Research Highlights-->
    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Research Highlights (<a href="https://scholar.google.com/citations?user=Q0F92XIAAAAJ&hl=en">[Full List]</a>)</h2>


            <h4><alert>Question Answering</alert></h4>
            <div class="paper"><img class="paper" src="./resources/paper_icon/naacl_demo_short.gif"
                title="">
                <div>We have developed question answering systems under different scenarios. 
                For Machine Reading Comprehension (MRC), we proposed different methods to improve the accuracy of answering questions over passages [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6458/6314">AAAI'20</a>; <a href="https://aclanthology.org/2023.findings-acl.391.pdf">ACL'23</a>]. 
                For Knowledge Graph Question Answering (KGQA), we investegated and improved different modules in the QA framework, which achieve SOTA results in a variety of scenarios (e.g., simple questions [<a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=5443&context=sis_research">IJCAI'19</a>], complex questions [<a href="https://arxiv.org/pdf/2101.03737">WSDM'20</a>; <a href="https://aclanthology.org/2020.acl-main.91.pdf">ACL'20</a>; <a href="https://arxiv.org/pdf/2306.06872">ACL'23</a>], conversational questions [<a href="https://aclanthology.org/2021.acl-long.255.pdf">ACL'21</a>]).
                For Question Answering with Databases, we explored to generate complex questions in a low-resource condition [<a href="https://arxiv.org/pdf/2310.08395">EMNLP'23</a>] and align Large Language Models (LLMs) to a domain-specific database [<a href="https://arxiv.org/pdf/2402.16567">ArXiv</a>].
                For Multi-modal Question Answering, we discussed the applications of LLMs and its safety issue [<a href="http://arxiv.org/abs/2311.09050.pdf">MM'23</a>; <a href="https://arxiv.org/pdf/2402.00357">IJCAI'24</a>;<a href="https://arxiv.org/pdf/2311.17600">ECCV'24</a>].<br>
                
                There are surveys [<a href="https://arxiv.org/pdf/2108.06688">TKDE'22</a>] you can start with.
                We have demonstration pages [<a href="sync-finqa.shuishan.net.cn/test">Demonstration Page</a>] that you can try on!
                </div>
                <div class="spanner">
                </div>
            </div> 

            <h4><alert>Educational NLP</alert></h4>
            <div class="paper"><img class="paper" src="./resources/paper_icon/coling2024-textsimplification.png"
                title="">
                <div>We have worked on the educational NLP tasks, such as Math Word Problem (MWP) solving [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/21723">AAAI demo'22</a>; <a href="https://arxiv.org/pdf/2305.04091">ACL'23</a>; <a href="https://arxiv.org/pdf/2310.16535">EMNLP Finding'23</a>], Chinese Spelling Check (CSC) [<a href="https://link.springer.com/chapter/10.1007/978-3-031-44699-3_29">NLPCC'23</a>], Grammatical Error Correction (GEC) [<a href="https://arxiv.org/pdf/2311.04906">CIKM'23</a>], Text Simplification [<a href="https://arxiv.org/abs/2402.14704">COLING'24</a>].<br>
                
                There are surveys [<a href="https://arxiv.org/pdf/2401.07518">ArXiv</a>] you can start with.
                </div>
                <div class="spanner">
                </div>
            </div>            
        </div>
    </div>
    <br>

    
    <!--Research Highlights-->
    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Selected Papers</h2>
            (* indicates equal contribution, # indicates corresponding author)

            <div class="paper">
    
            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2311.17600">Query-relevant images jailbreak large multi-modal models</a><br>
                Xin Liu, Yichen Zhu, Yunshi Lan#, Chao Yang#, Yu Qiao<br>
                ECCV, 2024 <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2402.00357">Safety of Multimodal Large Language Models on Images and Text</a><br>
                Xin Liu, Yichen Zhu, Yunshi Lan#, Chao Yang#, Yu Qiao<br>
                IJCAI, 2024 <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                FinQA: A Training-free Dynamic Knowledge Graph Question Answering System in Finance with LLM-based Revision<br>
                Wenbiao Tao, Hanlun Zhu*, Keren Tan, Jiani Wang, Yuanyuan Liang, Huihui Jiang, Pengcheng Yuan, Yunshi Lan#<br>
                ECML-PKDD, 2024 <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2401.07518">Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends</a><br>
                Yunshi Lan, Xinyuan Li, Hanyue Du, Xuesong Lu, Ming Gao, Weining Qian, Aoying Zhou<br>
                Preprint, 2024 <br>
                <a href="https://github.com/LiXinyuan1015/NLP-for-Education/invitations">[GitHub Page]</a> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2402.14704">An LLM-Enhanced Adversarial Editing System for Lexical Simplification</a><br>
                Keren Tan, Kangyang Luo, Yunshi Lan#, Zheng Yuan and Jinlong Shu<br>
                COLING, 2024 <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2310.16535">Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation</a><br>
                Yuanyuan Liang, Jianing Wang, Hanlun Zhu, Lei Wang, Yunshi Lan#, Weining Qian<br>
                EMNLP, 2023 <br>
                <a href="./resources/bibtex/emnlp2023_kbqg.txt">[BibTex]</a> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2310.16535">R3 Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context</a><br>
                Qingyuan Tian, Hanlun Zhu, Lei Wang, Yang Li, Yunshi Lan<br>
                EMNLP Finding, 2023 <br>
                <a href="./resources/bibtex/emnlp2023_kbqg.txt">[BibTex]</a><br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="http://arxiv.org/abs/2311.09050.pdf">Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts</a><br>
                Yunshi Lan, Alex Xiang Li, Xin Liu, Yang Li, Wei Qin, Weining Qian<br>
                ACM MM, 2023 <br>
                <a href="./resources/bibtex/mm2023_vqa.txt">[BibTex]</a><br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2311.04906">FlaCGEC: A Chinese Grammatical Error Correction Dataset with Fine-grained Linguistic Annotation</a><br>
                Hanyue Du, Yike Zhao, Qingyuan Tian, Jiani Wang, Lei Wang, Yunshi Lan#, Xuesong Lu<br>
                ACM CIKM, 2023 <br>
                <a href="./resources/bibtex/cikm2023_cgec.txt">[BibTex]</a>
                <a href="https://github.com/hyDududu/FlaCGEC">[GitHub Page]</a><br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2306.06872">History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling</a><br>
                Hao Sun, Yang Li, Liwei Deng, Bowen Li, Binyuan Hui, Binhua Li, Yunshi Lan, Yan Zhang, Yongbin Li<br>
                ACL, 2023 <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://aclanthology.org/2023.findings-acl.391.pdf">Structure-discourse hierarchical graph for conditional question answering on long documents</a><br>
                Haowei Du, Yansong Feng, Chen Li, Yang Li, Yunshi Lan, Dongyan Zhao<br>
                ACL Finding, 2023 <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2305.04091">Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models</a><br>
                Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, Ee-Peng Lim<br>
                ACL, 2023 <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21723">MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers</a><br>
                Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan#, Bing Tian Dai, Yan Wang, Dongxiang Zhang, Ee-Peng Lim<br>
                AAAI, 2022 <br>
                <a href="https://mp.weixin.qq.com/s/2lLInVAMZ7s_8pc1A5Czjg">[中文解读]</a>
                <a href="https://github.com/LYH-YF/MWPToolkit">[GitHub Page]</a>
                <a href="./resources/bibtex/aaai2022_demo.txt">[BibTex]</a> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2108.06688">Complex Knowledge Base Question Answering: A Survey</a><br>
                Yunshi Lan, Gaole He*, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao and Ji-Rong Wen<br>
                TKDE, 2022 <br>
                <a href="https://mp.weixin.qq.com/s/ZYGsB4AhgLyU0kuKWn9OlQ">[中文解读]</a>
                <a href="https://github.com/RUCAIBox/Awesome-KBQA">[GitHub Page]</a>
                <a href="./resources/bibtex/tkde2022_survey.txt">[BibTex]</a> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2108.06688">Modeling Transitions of Focal Entities for Conversational knowledge Base Question Answering</a><br>
                Yunshi Lan, Jing Jiang<br>
                ACL, 2021 <br>
                <a href="./resources/bibtex/acl2021_ckbqa.txt">[BibTex]</a> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://arxiv.org/pdf/2101.03737">Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals</a><br>
                Gaole He, Yunshi Lan, Jing Jiang, Xin Zhao, Ji-Rong Wen<br>
                WSDM, 2021 <br>
                <a href="https://github.com/RichardHGL/WSDM2021_NSM">[GitHub Page]</a>
                <a href="./resources/bibtex/wsdm2021_ckbqa.txt">[BibTex]</a> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://aclanthology.org/2020.acl-main.91.pdf">Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases</a><br>
                Yunshi Lan, Jing Jiang<br>
                ACL, 2020 <br>
                <a href="https://github.com/lanyunshi/Multi-hopComplexKBQA">[GitHub Page]</a>
                <a href="./resources/bibtex/acl2020_ckbqa.txt">[BibTex]</a> <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=5904&context=sis_research">Knowledge base question answering with a matching-aggregation model and question-specific contextual relations</a><br>
                Yunshi Lan, Shuohang Wang, Jing Jiang<br>
                IEEE/ACM TASLP, 2019 <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=5443&context=sis_research">Knowledge base question answering with topic units</a><br>
                Yunshi Lan, Shuohang Wang, Jing Jiang<br>
                IJCAI, 2019 <br>
                <br>
                </li>
            </ul>

            <ul>
                <li>
                <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=5283&context=sis_research">Embedding WordNet knowledge for textual entailment</a><br>
                Yunshi Lan, Jing Jiang<br>
                COLING, 2018 <br>
                <br>
                </li>
            </ul>

        </div>
    </div>
    <br>

    <!--Grants-->
    <div style="clear: both;">
        <div class="section">
            <h2>Grants
            </h2>
            <div class="paper">
                <ul>
                    <li>
                        Jan. 2023 - Dec. 2025: Knowledge Base Question Answering System in Interactive Environments<br>
                        National Science Foundation of China, PI<br>
                    </li>
                </ul>
                <div class="spanner"></div>
                <ul>
                    <li>
                        Jan. 2024 - Dec. 2027: Intelligent Low Carbon Oriented Theories and Critical Techniques for Perception and inference of Cross-Border Electric Power Trading Big Data<br>
                        National Science Foundation of China, ECNU PI<br>
                    </li>
                </ul>
                <div class="spanner"></div>
                <ul>
                    <li>
                        Oct. 2022 - Sep. 2024: Knowledge Base Construction and Question Answering for an Education Intelligent Platform<br>
                        Shanghai Pujiang Talent Program, PI<br>
                    </li>
                </ul>
                <div class="spanner"></div>
                <ul>
                    <li>
                        July. 2022 - Dec. 2023: Online Education Toolkit and Platform Development for International Chinese Education<br>
                        East China Normal University, PI (Completed)<br>
                    </li>
                </ul>
                <div class="spanner"></div>
                <ul>
                    <li>
                        Aug. 2023 - Dec. 2023: Financial KBQA collaborating with Large Language Models<br>
                        Joint Reseach Program with Guotai Junan Securities, PI (Completed)<br>
                    </li>
                </ul>
                <div class="spanner"></div>
                <ul>
                    <li>
                        Sep. 2023 - Sep. 2024: Question Answering System in PolarDB Community<br>
                        Joint Reseach Program with Alibaba Cloud, ECNU Co-PI<br>
                    </li>
                </ul>
                <ul>
                    <li>
                        Jan. 2024 - Dec. 2025: Researches on International Education of Chinese and Computer Science with LLMs<br>
                        East China Normal University, PI<br>
                    </li>
                </ul>
                <ul>
                    <li>
                        Jan. 2024 - Dec. 2028: Researches on Knowledge Mining based on Scientific Data and Literature <br>
                        National Science Foundation of China, ECNU PI<br>
                    </li>
                </ul>
                <div class="spanner"></div>
                <br>
            </div>
        </div>
    </div>

    <!--Teaching-->
    <div style="clear: both;">
        <div class="section">
            <h2>Teaching
            </h2>
            <div class="paper">
                <ul>
                    <li>
                        Deep Learning<br>
                        for both undergraduate students and postgraduate students<br>
                        2024 Spring <a href="./resources/html/deep-learning2023-2024.html">Deep Learning (Undergraduate)</a> 
                    </li>
                </ul>
                <div class="spanner"></div>
                <br>
            </div>
        </div>
    </div>

    <!--Professional Service-->
    <div style="clear: both;">
        <div class="section">
            <h2>Professional Service
            </h2>
            <div class="paper">
                <ul>
                    <li>
                        I have served as a session chair in EMNLP 2023<br>
                    </li>
                    <li>
                        I regularly serve as the reviewer of ACL, EMNLP, AAAI, IJCAI<br>
                    </li>
                </ul>
                <div class="spanner"></div>
                <br>
            </div>
        </div>
    </div>

    <!--Invited Talk-->
    <div style="clear: both;">
        <div class="section">
            <h2>Invited Talk
            </h2>
            <div class="paper">
                <ul>
                    <li>
                        Invited Talk on Applications of Large Language Models on Finance Domain.<br>
                        ECNU (2023.12)<br>
                    </li>
                    <li>
                        Invited Talk on Knowledge-based Question Answering with Large Language Models. <a href="./resources/slides/nust-pre.pdf">[Slides]</a> <br>
                        NUST, NLP Group (2023.11)<br>
                        SCU, NLP Group (2023.12)
                    </li>
                </ul>
                <div class="spanner"></div>
                <br>
            </div>
        </div>
    </div>

    <!--Mentoring-->
    <div style="clear: both;">
        <div class="section">
            <h2>Mentoring Experience</h2>
            <div class="paper">
                <h4>Master students</h4>
                <ul>
                    <li>
                        Zeyu Sheng (CCL 2023) (ECNU -> ByteDance, Shanghai); Xuyao Hu (ECNU -> Xiecheng, Shanghai); Yuze Huang (ECNU -> Xiecheng, Shanghai); Yuxuan Huang (ECNU -> Shanghai Stock Exchange, Shanghai)
                    </li>
                    <li>
                        Keren Tan (中文信息学报; COLING 2024; ECML-PKDD 2024; Shanghai excellent graduate student 2024) (ECNU -> Guotai Junan Securities, Shanghai); Jiani Wang (CIKM 2023; ECML-PKDD 2024) (ECNU -> Midea, Shanghai); Lei Pan(ECNU -> AISPEECH, Shanghai)
                    </li>
                </ul>
                <h4>Undergraduate students</h4>
                <ul>
                    <li>
                        Qingyuan Tian (CIKM 2023; EMNLP Finding 2023) (ECNU -> PhD candidate, SJTU)<br>
                    </li>
                </ul>
                <div class="spanner"></div>
                <br>
            </div>
        </div>
    </div>

</body>

</html>
