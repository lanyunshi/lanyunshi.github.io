
<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>深度学习</title>
    <meta name="description" content="">
    <meta name="author" content="">
    <link href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900" rel="stylesheet">
    <link href="../css/style.css" rel="stylesheet" media="screen">
    <link href="../css/style.css" rel="stylesheet">

<style type="text/css">
  body {
    padding-top: 0px;
    padding-bottom: 0px;
  }
  .sidebar-nav {
    padding: 0px 0;
  }
  .page-header {
    border-bottom: 1px solid #ADA9A9;
    margin-bottom: 10px;
  }

  .topic-column {
    column-width: 80em;
  }

  .recommended-column {
    column-width: 100em;
  }
</style>

<style type="text/css">
</style>

</head>

<body>

    <!--Research Interest-->
    <div style="clear: both;">
        <div class="section">
            <h2>深度学习
            </h2>
            <div class="paper">
              课程： 华东师范大学（2024-2025学年第二学期）<br>
              授课教师： <a href="https://lanyunshi.github.io">兰韵诗</a> （数据科学与工程学院）; 邮箱：yslan [at] dase.ecnu.edu.cn<br>
              理论课： 周一 3-4节 文史楼203<br>
              实验课： 周一 11-12节 中北三馆教学楼201<br>
              </tr>
            </div>

            <div class="paper">
              <li>
                《深度学习》是面向大数据专业开设的专业选修课。深度学习的概念源于人工神经网络的研究。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。本课程主要介绍深度学习的理论基础和实践。通过本课程的学习，计划使学生掌握深度学习的核心思想，以及主要技术，包括深度前馈网络，随机梯度下降，自适应学习算法，卷积神经网络，循环神经网络，递归神经网络，长短期记忆，注意力机制,和无监督特征学习方法等。与此同时，科普一些深度学习的扩展模型，包括序列生成模型，深度生成模型，大型预训练语言模型等。<br>
              </li>
              <li>
                本课程采用的教学素材大部分来自于复旦大学邱锡鹏老师<a href="https://nndl.github.io">《神经网络与深度学习》</a>。后期章节内容会根据课程的安排进行调整和修改，今年的课程我们添加了更多关于大语言模型的相关内容并结合实验室的项目实践整理成章节。<br>
              </li>
              <li>
                本课程配套实验课程，能够结合理论让大家全方位掌握深度学习的知识。普通程序题和期末考察题均通过<a href="https://www.shuishan.net.cn">水杉在线</a>的天梯系统发布，请同学们关注。<br>
              </li>
            </div>

            <div class="paper">
              相关书籍推荐<br>
              <li>
                邱锡鹏，<a href="https://nndl.github.io">《神经网络与深度学习》</a>。
              </li>
              <li>
                Ian Goodfellow等，<a href="https://github.com/MingchaoZhu/DeepLearning/releases/download/v0.0.1/DL_cn.pdf">《深度学习》</a>。
              </li>
              <li>
                阿斯顿·张等，<a href="https://d2l.ai/">《动手学深度学习》</a>。
              </li>
              <li>
                Bishop, C.M.，<a href="https://link.springer.com/book/9780387310732">《Pattern recognition and Machine Learning》</a>。
              </li>
              <li>
                张奇等，<a href="https://intro-llm.github.io">《大规模语言模型：从理论到实践》</a>。
              </li>
              <li>
                赵鑫等，<a href="https://llmbook-zh.github.io">《大语言模型》</a>。
              </li>
            </div>

            <div class="paper">
              第一周<br>
              深度学习课程简介; 深度学习概述 ｜ 熟悉水杉在线;Python基础上手 （2月17日）<br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week1.pdf">[讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/lab_system.pdf">[天梯系统使用手册]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/week1_lab.pdf">[实验讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/test_solution.py">[实验答案]</a><br>
              第二周<br>
              线性模型 ｜ 回归模型实现-实践作业 （2月24日）<br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week2.pdf">[讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/week2_lab.pdf">[实验讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/lab2.zip">[实验内容]</a>;<br>
              第三周<br>
              线性分类模型 ｜ 线性分类模型实现 （3月3日）<br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week3.pdf">[讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/week3_lab.pdf">[实验讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/lab3.zip">[实验内容]</a>;<br>
              第四周<br>
              前馈神经网络 ｜ 前馈神经网络 （3月10日）<br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week4.pdf">[讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/week4_lab.pdf">[实验讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/lab4.zip">[实验内容]</a><br>
              第五周<br>
              卷积神经网络 ｜ 卷积神经网络实现-实践作业  （3月17日）<br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week5.pdf">[讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/week5_lab.pdf">[实验讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/lab5.zip">[实验内容]</a> <br>
              第六周<br>
              卷积神经网络和循环神经网络 ｜（3月24日）<br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week6.pdf">[讲义]</a>; <br>
              第七周<br>
              循环神经网络 ｜循环神经网络实现-实践作业  （3月31日）<br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week7_lab.pdf">[实验讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/lab6.zip">[实验内容]</a>; <br>
              第八周<br>
              网络优化与正则化 ｜ 优化的梯度更新实现（4月7日） <br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week8.pdf">[讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/week8_lab.pdf">[实验讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/lab7.zip">[实验内容]</a> <br>
              第九周<br>
              记忆与注意力机制 ｜ 记忆与注意力机制-实践作业（4月14日）<br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week9.pdf">[讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/week9_lab.pdf">[实验讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/lab8.zip">[实验内容]</a> <br>
              第十周<br>
              独立学习方式 ｜ BERT的简单实现-实践作业（4月21日）<br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week10.pdf">[讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/week10_lab.pdf">[实验讲义]</a> <br>
              第十一周<br>
              无监督学习 ｜ （4月27日） <br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week11.pdf">[讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/week11_lab.pdf">[实验讲义]</a> <br>
              概率图模型 ｜ 期末考察题公布（4月28日） <br>
              <a href="https://lanyunshi.github.io/resources/slides/dl/week12.pdf">[讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/week12_lab.pdf">[实验讲义]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/lab10.zip">[实验内容(代码)]</a>; <a href="https://lanyunshi.github.io/resources/slides/dl/lab10_data.zip">[实验内容(数据)]</a><br>
              第十二周<br>
              五一放假 （5月5日）<br>
              第十三周<br>
              深度生成模型 ｜（5月12日）<br>
              <a href="">[讲义]</a>; <br>
              第十四周<br>
              预训练语言模型概要 ｜（5月19日）<br>
              <a href="">[讲义]</a>;<br>
              第十五周<br>
              大语言模型工作原理、评测和提示学习 ｜（5月26日）<br>
              <a href="">[讲义]</a>;<br>
              第十六周<br>
              端午放假 ｜（6月2日）<br>
              第十七周<br>
              大语言模型的微调和人类对齐 ｜（6月9日）<br>
              <a href="">[讲义]</a>;<br>
              第十八周<br>
              大语言模型在语言教育中的应用实践 ｜ 期末考察题目分享汇报（6月16日）<br>
              <a href="">[讲义]</a>;<br>
            </div>

            <div class="paper">
              理论课随堂表现（10%）<br>
              实践作业（50%）<br>
              期末考察（40%）<br>
              </tr>
            </div>

            <div class="paper">
              本课程感谢本组助教同学、陆雪松老师组天梯系统的开发及管理同学、阿里云PolarDB团队的支持！
            </div>
        </div>
    </div>



</body>

</html>

